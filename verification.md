# Data Verification

Since this is obviously a contentious topic with contradictory claims being
made by different parties, here is a description of the data used in this
analysis and how anyone can verify it.

## Graphs

To regenerate graphs of the data used in this analysis, run `run-server.sh` from
the root of this project. Then point your browser at
[`http://localhost:8000/graphs/hist_good_and_bad_overlayed.html`](http://localhost:8000/graphs/hist_good_and_bad_overlayed.html)
(or any of the other html files in the `graphs` folder) to regenerate the graphs
from the raw data.

## data/block-times-2023-01.csv.gz

This file is zipped because of GitHub's size limitations. Here are the first
22 lines of the unzipped data:

```csv
height,creationtime,chainid,minable_time,delta
3337165,2023-01-01T03:59:18.356108+00:00,1,,
3337165,2023-01-01T03:59:23.251404+00:00,17,,
3337165,2023-01-01T03:59:29.153163+00:00,7,,
3337165,2023-01-01T03:59:30.162961+00:00,12,,
3337165,2023-01-01T03:59:31.334097+00:00,13,,
3337165,2023-01-01T03:59:32.580523+00:00,2,,
3337165,2023-01-01T03:59:35.169517+00:00,8,,
3337165,2023-01-01T03:59:35.811797+00:00,10,,
3337165,2023-01-01T03:59:35.886084+00:00,3,,
3337165,2023-01-01T03:59:38.604218+00:00,5,,
3337165,2023-01-01T03:59:38.647356+00:00,18,,
3337165,2023-01-01T03:59:40.574589+00:00,11,,
3337165,2023-01-01T03:59:45.206795+00:00,6,,
3337165,2023-01-01T03:59:45.647418+00:00,9,,
3337165,2023-01-01T03:59:50.160239+00:00,16,,
3337165,2023-01-01T03:59:57.320584+00:00,14,,
3337165,2023-01-01T04:00:00.934788+00:00,19,,
3337165,2023-01-01T04:00:03.492379+00:00,15,,
3337165,2023-01-01T04:00:08.137002+00:00,4,,
3337165,2023-01-01T04:00:22.618477+00:00,0,,
3337166,2023-01-01T03:59:45.443756+00:00,11,2023-01-01T03:59:40.574589+00:00,4.869167
```

This file contains one month of real block data from the Kadena blockchain
from January 2023. The first three columns of this table can be verified
directly from the Kadena blockchain. Let's take line 22 as an example (the
first line with all 5 fields). The third column has a chain id of 11 and the
first column has a block height of 3337166. Make these two numbers into a
Kadena block explorer link as follows:

```
https://explorer.chainweb.com/mainnet/chain/CHAINID/height/BLOCKHEIGHT
```

In this case that would be
[https://explorer.chainweb.com/mainnet/chain/11/height/3337166](https://explorer.chainweb.com/mainnet/chain/11/height/3337166).
On that page we can see that the creation time is `2023-01-01 03:59:45.443756
UTC` which matches the `creationtime` column in the file.

The last two columns `minable_time` and delta are created using the method from
DesheShai's `add_layer` function. The `minable_time` column is the max creation
time of that chain and all three of its neighbors according to the Kadena
chain graph. At the above explorer link we can see the neighbor chains for
chain 11 are chains 1, 10, and 12. So we look at the creation times for those
four chains at the previous block height, 3337165.

```
Chain 11: 2023-01-01T03:59:40.574589+00:00
Chain 1:  2023-01-01T03:59:18.356108+00:00
Chain 10: 2023-01-01T03:59:35.811797+00:00
Chain 12: 2023-01-01T03:59:30.162961+00:00
```

The latest of these times (`max(...cd[v]["neighbors"]])` in [DesheShai's
code](code-orig.py#L66)) is `03:59:40.574589`, which is exactly what we see in the
`minable_time` column on that line.  To get the last column, `delta`, we
subtract this minable time from the `creationtime` (column 2) on that row.
This delta value is a number of seconds. When we divide it by the block time
of 30 seconds, we get the value that DesheShai is erroneously using `Exp()` to
approximate.

## data/stalls-real.csv

This file contains the last column (delta) of `data/block-times-2023-01.csv`.
You can generate it yourself as follows:

```
duckdb -c "COPY (SELECT 'real stall' as type, round(delta, 3) as value FROM 'data/block-times-2023-01.csv' WHERE delta IS NOT NULL) TO 'stalls-real.csv' WITH (HEADER true, DELIMITER ',')"
```

## data/stalls-bad-random.csv

This file contains the actual random stall values generated by the original
simulation code [using a fixed random seed of 42](code-fixed.py#L9). There are
two ways you can verify this. The first is to modify the original code to print
the stall value after every call to `Exp()`. This proves that the values being
printed are the exact some ones used by the simulation. Alternatively, a faster
way to do this is to call `Exp()` 460,000 times and print all the values. The
code for both of these methods is in this repo and you can run them with either
one of the following commands:

```
python code-save-random.py > stalls-bad-random.csv
python save-exp-random.py > stalls-bad-random.csv
```

## data/stalls-resampled.csv

This file contains randomly generated stalls using the same probability
distribution as the real stall data. You can regenerate this data with this
command:

```
python resample.py
```

This program reads the real data from `data/stalls-real.csv` and [constructs a
kernel-density estimate](resample.py#L17) (KDE) of the probability density
function. Then it generates random samples from that KDE. This is a well-known
statistical technique [provided by the popular Python scipy
library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html).

## data/hist_both.csv

This file is used to create the histogram images. It contains 460,000 lines of
the `delta` field from the above `data/block-times-2023-01.csv.gz` file (labeled
"real stall" in first column) followed by 460,000 lines of the data from
`stalls-bad-random.csv` (labeled "bad stall" in the first column).

[The first "real stall" value](data/hist_both.csv#L2) is 4.869 which matches the
first value that we saw above (rounded to 3 decimal places).

The following commands will regenerate `hist_both.csv`:

```
head -n 460001 stalls-real.csv > hist_both.csv
cat stalls-bad-random.csv >> hist_both.csv
```
